from robustbench.data import load_cifar10
from robustbench.utils import load_model
import foolbox as fb
from robustbench.data import load_cifar10c
from robustbench.utils import clean_accuracy
from autoattack import AutoAttack


if __name__ == '__main__':

    x_test, y_test = load_cifar10(n_examples=50) # x_test -> an input(image), y_test -> corresponding label

    # print("Torch version:", torch.__version__)
    # print("Is CUDA enabled?", torch.cuda.is_available())

    model = load_model(model_name='Carmon2019Unlabeled', dataset='cifar10', threat_model='Linf')
    model_fb = fb.PyTorchModel(model, bounds=(0, 1)) # Foolbox-wrapped version of my PyTorch model
    """PyTorchModel is a function from FoolBox that takes the PyTorch model and defines the expected input range for the images.
     Here, bounds=(0, 1) specifies that the pixel values are expected to be between 0 and 1 (normalized)."""
    _, advs, success = fb.attacks.LinfPGD(rel_stepsize=0.1, steps=20)(model_fb, x_test.to('cuda:0'), y_test.to('cuda:0'), epsilons=[8/255])
    print('Robust accuracy: {:.1%}'.format(1 - success.float().mean()))

    """_: This is typically a placeholder variable used to ignore a value that is not needed. In this context, it might be used to ignore the intermediate results of the attack.
        advs: This variable stores the generated adversarial examples
        success: This variable is a boolean array indicating whether each adversarial example in advs was successful in fooling the model"""

    print("Applying 'apgd-ce' and 'apgd-dlr' attacks:\n")
    adversary = AutoAttack(model, norm='Linf', eps=8 / 255, version='custom', attacks_to_run=['apgd-ce', 'apgd-dlr'])
    #adversary = AutoAttack(model, norm='Linf', eps=8 / 255)
    adversary.apgd.n_restarts = 1
    x_adv = adversary.run_standard_evaluation(x_test, y_test)

    corruptions = ['fog']
    x_test, y_test = load_cifar10c(n_examples=1000, corruptions=corruptions, severity=5)
    # Cifar10Corrupted is a dataset generated by adding 15 common corruptions + 4 extra corruptions to the test images in the Cifar10 dataset
    # x_test contains the corrupted test images and y_test contains the corresponding labels

    for model_name in ['Standard', 'Engstrom2019Robustness', 'Rice2020Overfitting', 'Carmon2019Unlabeled']:
        model = load_model(model_name, dataset='cifar10', threat_model='Linf')
        acc = clean_accuracy(model, x_test, y_test) # accuracy of the model on the corrupted test data(x_test) with their true labels(y_test)
        # clean_accuracy => in this case model performance on corrupted data
        print(f'Model: {model_name}, CIFAR-10-C accuracy: {acc:.1%}')


